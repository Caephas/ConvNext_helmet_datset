{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found ✅\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set your dataset directory (update this path based on where your dataset is stored)\n",
    "DATASET_PATH = \"helmet_dataset\"\n",
    "\n",
    "# Define annotation and image paths\n",
    "ANNOTATIONS_DIR = os.path.join(DATASET_PATH, \"annotations\")\n",
    "IMAGES_DIR = os.path.join(DATASET_PATH, \"images\")\n",
    "\n",
    "# Check if dataset exists\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(\"Dataset found ✅\")\n",
    "else:\n",
    "    print(\"Dataset not found ❌. Check your path!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import xmltodict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations converted and saved to helmet_dataset/helmet_annotations.csv ✅\n"
     ]
    }
   ],
   "source": [
    "def parse_voc_xml(xml_file):\n",
    "    with open(xml_file) as f:\n",
    "        data = xmltodict.parse(f.read())\n",
    "\n",
    "    filename = data[\"annotation\"][\"filename\"]\n",
    "    objects = []\n",
    "\n",
    "    for obj in data[\"annotation\"].get(\"object\", []):\n",
    "        if isinstance(obj, dict):  # Handle multiple objects\n",
    "            name = obj[\"name\"]\n",
    "            bbox = obj[\"bndbox\"]\n",
    "            xmin, ymin, xmax, ymax = int(bbox[\"xmin\"]), int(bbox[\"ymin\"]), int(bbox[\"xmax\"]), int(bbox[\"ymax\"])\n",
    "            objects.append([filename, name, xmin, ymin, xmax, ymax])\n",
    "    \n",
    "    return objects\n",
    "\n",
    "# Process all XML files\n",
    "data = []\n",
    "for xml_file in os.listdir(ANNOTATIONS_DIR):\n",
    "    if xml_file.endswith(\".xml\"):\n",
    "        data.extend(parse_voc_xml(os.path.join(ANNOTATIONS_DIR, xml_file)))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"filename\", \"class\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = os.path.join(DATASET_PATH, \"helmet_annotations.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Annotations converted and saved to {csv_path} ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelmetDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = 1 if self.img_labels.iloc[idx, 1] == \"helmet\" else 0  # 1 for helmet, 0 for no helmet\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize for ConvNeXt\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded: 19845 train images, 4962 test images ✅\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = HelmetDataset(csv_path, IMAGES_DIR, transform=transform)\n",
    "\n",
    "# Split into train & test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Dataset Loaded: {len(train_dataset)} train images, {len(test_dataset)} test images ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caephas/Downloads/School-Projects/ConvNext_helmet_datset/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/caephas/Downloads/School-Projects/ConvNext_helmet_datset/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Tiny_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /Users/caephas/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXt model loaded and modified for helmet detection ✅\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained ConvNeXt model\n",
    "model = models.convnext_tiny(pretrained=True)\n",
    "\n",
    "# Modify the final classification layer (from 1000 classes → 2 classes)\n",
    "num_ftrs = model.classifier[2].in_features\n",
    "model.classifier[2] = nn.Linear(num_ftrs, 2)  # 2 output classes (helmet/no helmet)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")  # Use MPS for Apple Silicon\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"ConvNeXt model loaded and modified for helmet detection ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Loss: 0.2636, Accuracy: 90.18%, Time: 1205.87s\n",
      "Epoch [2/10] - Loss: 0.2040, Accuracy: 91.51%, Time: 1157.84s\n",
      "Epoch [3/10] - Loss: 0.1854, Accuracy: 91.60%, Time: 1160.85s\n",
      "Epoch [4/10] - Loss: 0.1750, Accuracy: 91.71%, Time: 1161.94s\n",
      "Epoch [5/10] - Loss: 0.1656, Accuracy: 91.79%, Time: 1151.21s\n",
      "Epoch [6/10] - Loss: 0.1615, Accuracy: 91.81%, Time: 1155.89s\n",
      "Epoch [7/10] - Loss: 0.1593, Accuracy: 92.08%, Time: 1176.61s\n",
      "Epoch [8/10] - Loss: 0.1551, Accuracy: 91.98%, Time: 2729.15s\n",
      "Epoch [9/10] - Loss: 0.1554, Accuracy: 92.23%, Time: 1175.75s\n",
      "Epoch [10/10] - Loss: 0.1510, Accuracy: 92.26%, Time: 1187.71s\n",
      "Training complete ✅\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move to GPU/MPS\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%, Time: {end_time-start_time:.2f}s\")\n",
    "\n",
    "    print(\"Training complete ✅\")\n",
    "\n",
    "# Start training\n",
    "train_model(model, train_loader, criterion, optimizer, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.25% ✅\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradient updates during testing\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {acc:.2f}% ✅\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as helmet_detection_convnext.pth ✅\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "torch.save(model.state_dict(), \"helmet_detection_convnext.pth\")\n",
    "print(\"Model saved as helmet_detection_convnext.pth ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
